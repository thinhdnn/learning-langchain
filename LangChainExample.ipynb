{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "LZRbd9XcWfk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W79hTcvaWdLK"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community torch pypdf gpt4all faiss-gpu openai openpyxl docx beautifulsoup4 python-docx langchain-openai psycopg2-binary tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Ollama and pull model"
      ],
      "metadata": {
        "id": "3hv8OJkhfEiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "qlScykqybEay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "id": "JIYr05mudxOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama2-uncensored"
      ],
      "metadata": {
        "id": "Z1QKgOs5bITl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector database"
      ],
      "metadata": {
        "id": "wHTCNT0LWnk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, CSVLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from  langchain_community.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "\n",
        "dataPath = '/content/drive/MyDrive/data'\n",
        "vectorDBPath = 'stores/db_faiss'\n",
        "\n",
        "\n",
        "class VectorDatabase:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def createFromFiles(self, dataPath, vectorDBPath):\n",
        "        # Load data from files\n",
        "        dirLoader = DirectoryLoader(dataPath, glob = '*.csv', loader_cls = CSVLoader)\n",
        "        documents = dirLoader.load()\n",
        "\n",
        "        # Split text into characters\n",
        "        textSplitters = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "        chunks = textSplitters.split_documents(documents)\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = GPT4AllEmbeddings(model_file = 'models/all-MiniLM-L6-v2')\n",
        "        db = FAISS.from_documents(chunks, embeddings)\n",
        "        db.save_local(vectorDBPath)\n",
        "\n",
        "        return db\n",
        "\n",
        "    def createFromDir(self, dataPath, vectorDBPath):\n",
        "        # Load data from files\n",
        "        dirLoader = DirectoryLoader(dataPath)\n",
        "        documents = dirLoader.load()\n",
        "\n",
        "        # Split text into characters\n",
        "        textSplitters = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "        chunks = textSplitters.split_documents(documents)\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = GPT4AllEmbeddings(model_file = 'models/all-MiniLM-L6-v2')\n",
        "        db = FAISS.from_documents(chunks, embeddings)\n",
        "        db.save_local(vectorDBPath)\n",
        "\n",
        "        return db\n",
        "\n",
        "\n",
        "\n",
        "vectorDb = VectorDatabase()\n",
        "vectorDb.createFromFiles(dataPath = dataPath, vectorDBPath = vectorDBPath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsmXbGsbWqAo",
        "outputId": "7a7007a7-6b3a-4f30-9a76-49749fb03023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 45.9M/45.9M [00:00<00:00, 79.5MiB/s]\n",
            "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 553MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application Chain"
      ],
      "metadata": {
        "id": "hbZIlcaig31E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from  langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "\n",
        "# Load LLM\n",
        "\n",
        "class AppChain:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def loadModel(self, model):\n",
        "        llm = Ollama(\n",
        "            model = model,\n",
        "            callback_manager=CallbackManager(\n",
        "                [StreamingStdOutCallbackHandler()],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        return llm\n",
        "\n",
        "    def createPrompt(self, context, template):\n",
        "        prompt = PromptTemplate(template = template, input_variables = [\"context\", \"question\"])\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def createChain(self, prompt, llm, db):\n",
        "        llmChain = RetrievalQA.from_chain_type(\n",
        "            llm = llm,\n",
        "            chain_type = 'stuff',\n",
        "            chain_type_kwargs = {'prompt' : prompt},\n",
        "            retriever = db.as_retriever(search_kwargs = {'k': 3}, max_new_tokens = 1024),\n",
        "            return_source_documents = False,\n",
        "        )\n",
        "\n",
        "        return llmChain\n",
        "\n",
        "    def readVectorDB(self, vectorDBPath, model):\n",
        "\n",
        "        # Embeddings\n",
        "        embedding = GPT4AllEmbeddings(model_file = model)\n",
        "        db = FAISS.load_local(vectorDBPath, embedding, allow_dangerous_deserialization = True)\n",
        "\n",
        "        return db\n",
        "\n",
        "\n",
        "# Load chain and model\n",
        "\n",
        "chain = AppChain()\n",
        "llm = chain.loadModel(\"llama2-uncensored\")\n",
        "db = chain.readVectorDB('stores/db_faiss', 'models/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "# Geneate prompt\n",
        "promtTemplate = \"\"\"<|im_start|>system\n",
        "se the following pieces of context to answer the users question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "{context}\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Create prompt\n",
        "prompt = chain.createPrompt('context', promtTemplate)\n",
        "\n",
        "# Run chain\n",
        "llmChain = chain.createChain(prompt, llm, db)\n",
        "reponse = llmChain.invoke({'query': 'Explain about re-route ?'})\n",
        "print(reponse)"
      ],
      "metadata": {
        "id": "dHrG7U5Gg5wL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}